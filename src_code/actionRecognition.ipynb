{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data for the model\n",
    "\n",
    "There is too much data for us to read all at once in memory. We can use some built in functions in Keras to automatically process the data, generate a flow of batches from a directory, and also manipulate the images.\n",
    "\n",
    "### Image Manipulation\n",
    "\n",
    "Its usually a good idea to manipulate the images with rotation, resizing, and scaling so the model becomes more robust to different images that our data set doesn't have. We can use the **ImageDataGenerator** to do this automatically for us. Check out the documentation for a full list of all the parameters you can use here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_gen = ImageDataGenerator(#rotation_range=30, # rotate the image 30 degrees\n",
    "                               #width_shift_range=0.1, # Shift the pic width by a max of 10%\n",
    "                               #height_shift_range=0.1, # Shift the pic height by a max of 10%\n",
    "                               rescale=1/255, # Rescale the image by normalzing it.\n",
    "                               #shear_range=0.2, # Shear means cutting away part of the image (max 20%)\n",
    "                               #zoom_range=0.2, # Zoom in by 20% max\n",
    "                               horizontal_flip=True, # Allo horizontal flipping\n",
    "                               #fill_mode='nearest' # Fill in missing pixels with the nearest filled value\n",
    "                              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating many manipulated images from a directory\n",
    "\n",
    "\n",
    "In order to use .flow_from_directory, you must organize the images in sub-directories. This is an absolute requirement, otherwise the method won't work. The directories should only contain images of one class, so one folder per class of images.\n",
    "\n",
    "Structure Needed:\n",
    "\n",
    "* Image Data Folder\n",
    "    * Class 1\n",
    "        * 0.jpg\n",
    "        * 1.jpg\n",
    "        * ...\n",
    "    * Class 2\n",
    "        * 0.jpg\n",
    "        * 1.jpg\n",
    "        * ...\n",
    "    * ...\n",
    "    * Class n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 795 images belonging to 4 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras_preprocessing.image.DirectoryIterator at 0x7fde55c980b8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#image_gen.flow_from_directory('positions/train')\n",
    "image_gen.flow_from_directory('actions/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 200 images belonging to 4 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras_preprocessing.image.DirectoryIterator at 0x7fde54b1e6d8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_gen.flow_from_directory('actions/test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resizing Images\n",
    "\n",
    "Let's have Keras resize all the images to 150 pixels by 150 pixels once they've been manipulated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# width,height,channels\n",
    "image_shape = (150,150,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, Conv2D, MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=(3,3),input_shape=(150,150,3), activation='relu',))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=(3,3),input_shape=(150,150,3), activation='relu',))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=(3,3),input_shape=(150,150,3), activation='relu',))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# Dropouts help reduce overfitting by randomly turning neurons off during training.\n",
    "# Here we say randomly turn off 50% of neurons.\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Last layer, not binary!\n",
    "model.add(Dense(4))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', #not binary for the poses\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 148, 148, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 72, 72, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 34, 34, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 17, 17, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 18496)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               2367616   \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 516       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 2,424,452\n",
      "Trainable params: 2,424,452\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 795 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "\n",
    "train_image_gen = image_gen.flow_from_directory('actions/train',\n",
    "                                               target_size=image_shape[:2],\n",
    "                                               batch_size=batch_size,\n",
    "                                               class_mode='categorical') #catagoricalnfor the poses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(next(train_image_gen))\n",
    "#train_image_gen.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 200 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "test_image_gen = image_gen.flow_from_directory('actions/test',\n",
    "                                               target_size=image_shape[:2],\n",
    "                                               batch_size=batch_size,\n",
    "                                               class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fall': 0, 'sit': 1, 'stand': 2, 'walk': 3}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_image_gen.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "150/150 [==============================] - 167s 1s/step - loss: 1.0078 - acc: 0.5752 - val_loss: 0.6405 - val_acc: 0.7300\n",
      "Epoch 2/10\n",
      "150/150 [==============================] - 149s 996ms/step - loss: 0.5150 - acc: 0.8087 - val_loss: 0.5850 - val_acc: 0.8250\n",
      "Epoch 3/10\n",
      "150/150 [==============================] - 144s 960ms/step - loss: 0.2897 - acc: 0.8942 - val_loss: 0.4110 - val_acc: 0.8750\n",
      "Epoch 4/10\n",
      "150/150 [==============================] - 135s 902ms/step - loss: 0.1714 - acc: 0.9394 - val_loss: 0.5297 - val_acc: 0.8650\n",
      "Epoch 5/10\n",
      "150/150 [==============================] - 172s 1s/step - loss: 0.1232 - acc: 0.9594 - val_loss: 0.7046 - val_acc: 0.8750\n",
      "Epoch 6/10\n",
      "150/150 [==============================] - 145s 968ms/step - loss: 0.1065 - acc: 0.9661 - val_loss: 0.6324 - val_acc: 0.8850\n",
      "Epoch 7/10\n",
      "150/150 [==============================] - 109s 727ms/step - loss: 0.0679 - acc: 0.9779 - val_loss: 0.7049 - val_acc: 0.8650\n",
      "Epoch 8/10\n",
      "150/150 [==============================] - 103s 685ms/step - loss: 0.0729 - acc: 0.9775 - val_loss: 0.6486 - val_acc: 0.9000\n",
      "Epoch 9/10\n",
      "150/150 [==============================] - 120s 801ms/step - loss: 0.0576 - acc: 0.9815 - val_loss: 0.7077 - val_acc: 0.8900\n",
      "Epoch 10/10\n",
      "150/150 [==============================] - 114s 760ms/step - loss: 0.0614 - acc: 0.9833 - val_loss: 0.6704 - val_acc: 0.8800\n"
     ]
    }
   ],
   "source": [
    "results = model.fit_generator(train_image_gen,epochs=10,\n",
    "                              steps_per_epoch=150,\n",
    "                              validation_data=test_image_gen,\n",
    "                             validation_steps=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5748427674205548,\n",
       " 0.8088050317714299,\n",
       " 0.8943396228664326,\n",
       " 0.9392033544226512,\n",
       " 0.9597484277729218,\n",
       " 0.9660377359740139,\n",
       " 0.9777777777777777,\n",
       " 0.9773584905660377,\n",
       " 0.9815513628083955,\n",
       " 0.9832285115303984]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.history['acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fde229fa550>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(results.history['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = load_model('model1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting on new images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fall': 0, 'sit': 1, 'stand': 2, 'walk': 3}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_image_gen.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(802, 350, 3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = cv2.imread('POSE3.jpg')\n",
    "v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "\n",
    "pose_file = 'testing/walk5.jpg'\n",
    "\n",
    "pose = image.load_img(pose_file, target_size=(150, 150))\n",
    "\n",
    "pose = image.img_to_array(pose)\n",
    "\n",
    "pose = np.expand_dims(pose, axis=0) #so the network can think its a batch of one image\n",
    "pose = pose/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_prob = new_model.predict(pose)\n",
    "new_model.predict_classes(pose) \n",
    "                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.7743273e-06 3.3431544e-07 1.8688984e-07 3.5053063e-02]]\n"
     ]
    }
   ],
   "source": [
    "# Output prediction\n",
    "print(prediction_prob) #how sure is it that the image belongs to the array it chose "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "#function to classify a pose\n",
    "def predictPose(imageName):\n",
    "    #resize the image since the model is trained with 150 by 150 images\n",
    "    pose = image.load_img(imageName, target_size=(150,150))\n",
    "    pose = image.img_to_array(pose)\n",
    "    pose = np.expand_dims(pose, axis=0)\n",
    "    pose = pose/255\n",
    "    \n",
    "    #actual classification\n",
    "    prediction_prob = new_model.predict(pose)\n",
    "    #returns which pose\n",
    "    poseNumber = new_model.predict_classes(pose)\n",
    "    #print(prediction_prob)\n",
    "    return poseNumber[0]+1\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAllImages(rootdir):\n",
    "    \"\"\"Gets all of the images in a folder and runs them through OpenPose.\n",
    "    You just need to run this function with the folder of pose images\"\"\"\n",
    "    path, dirs, files = next(os.walk(rootdir))\n",
    "    file_count = len(files) - 1\n",
    "    poseList = []\n",
    "    action = 0\n",
    "    actionList = []\n",
    "    i = 0\n",
    "    for subdir, dirs, files in os.walk(rootdir):\n",
    "        for file in files:\n",
    "            # print os.path.join(subdir, file)\n",
    "            filepath = subdir + os.sep + file\n",
    "\n",
    "            if filepath.endswith(\".jpg\"):\n",
    "                i += 1\n",
    "                #printProgressBar(i + 1, file_count, prefix='Progress:', suffix='Complete', length=50)\n",
    "                print(\"processing image \"+ str(filepath))\n",
    "                action = predictPose(filepath)\n",
    "                print(action)\n",
    "                actionList.append(action)\n",
    "    return actionList\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time at start: 5.888938903808594e-05\n",
      "processing image testing/walk/63.jpg\n",
      "4\n",
      "processing image testing/walk/77.jpg\n",
      "1\n",
      "processing image testing/walk/62.jpg\n",
      "4\n",
      "processing image testing/walk/74.jpg\n",
      "4\n",
      "processing image testing/walk/60.jpg\n",
      "4\n",
      "processing image testing/walk/49.jpg\n",
      "4\n",
      "processing image testing/walk/61.jpg\n",
      "4\n",
      "processing image testing/walk/75.jpg\n",
      "4\n",
      "processing image testing/walk/59.jpg\n",
      "4\n",
      "processing image testing/walk/71.jpg\n",
      "3\n",
      "processing image testing/walk/65.jpg\n",
      "4\n",
      "processing image testing/walk/64.jpg\n",
      "4\n",
      "processing image testing/walk/70.jpg\n",
      "4\n",
      "processing image testing/walk/58.jpg\n",
      "4\n",
      "processing image testing/walk/8.jpg\n",
      "1\n",
      "processing image testing/walk/66.jpg\n",
      "4\n",
      "processing image testing/walk/72.jpg\n",
      "4\n",
      "processing image testing/walk/73.jpg\n",
      "1\n",
      "processing image testing/walk/67.jpg\n",
      "4\n",
      "processing image testing/walk/9.jpg\n",
      "3\n",
      "processing image testing/walk/14.jpg\n",
      "4\n",
      "processing image testing/walk/28.jpg\n",
      "3\n",
      "processing image testing/walk/29.jpg\n",
      "4\n",
      "processing image testing/walk/15.jpg\n",
      "1\n",
      "processing image testing/walk/17.jpg\n",
      "4\n",
      "processing image testing/walk/16.jpg\n",
      "1\n",
      "processing image testing/walk/12.jpg\n",
      "3\n",
      "processing image testing/walk/13.jpg\n",
      "4\n",
      "processing image testing/walk/39.jpg\n",
      "4\n",
      "processing image testing/walk/11.jpg\n",
      "4\n",
      "processing image testing/walk/10.jpg\n",
      "1\n",
      "processing image testing/walk/38.jpg\n",
      "4\n",
      "processing image testing/walk/21.jpg\n",
      "4\n",
      "processing image testing/walk/35.jpg\n",
      "4\n",
      "processing image testing/walk/34.jpg\n",
      "4\n",
      "processing image testing/walk/20.jpg\n",
      "4\n",
      "processing image testing/walk/36.jpg\n",
      "4\n",
      "processing image testing/walk/22.jpg\n",
      "4\n",
      "processing image testing/walk/23.jpg\n",
      "3\n",
      "processing image testing/walk/37.jpg\n",
      "1\n",
      "processing image testing/walk/33.jpg\n",
      "1\n",
      "processing image testing/walk/27.jpg\n",
      "4\n",
      "processing image testing/walk/26.jpg\n",
      "1\n",
      "processing image testing/walk/32.jpg\n",
      "4\n",
      "processing image testing/walk/18.jpg\n",
      "3\n",
      "processing image testing/walk/24.jpg\n",
      "4\n",
      "processing image testing/walk/30.jpg\n",
      "2\n",
      "processing image testing/walk/31.jpg\n",
      "3\n",
      "processing image testing/walk/25.jpg\n",
      "4\n",
      "processing image testing/walk/19.jpg\n",
      "4\n",
      "processing image testing/walk/42.jpg\n",
      "4\n",
      "processing image testing/walk/4.jpg\n",
      "4\n",
      "processing image testing/walk/56.jpg\n",
      "4\n",
      "processing image testing/walk/80.jpg\n",
      "4\n",
      "processing image testing/walk/5.jpg\n",
      "4\n",
      "processing image testing/walk/57.jpg\n",
      "4\n",
      "processing image testing/walk/43.jpg\n",
      "4\n",
      "processing image testing/walk/55.jpg\n",
      "4\n",
      "processing image testing/walk/7.jpg\n",
      "1\n",
      "processing image testing/walk/41.jpg\n",
      "4\n",
      "processing image testing/walk/69.jpg\n",
      "4\n",
      "processing image testing/walk/68.jpg\n",
      "4\n",
      "processing image testing/walk/40.jpg\n",
      "3\n",
      "processing image testing/walk/54.jpg\n",
      "4\n",
      "processing image testing/walk/6.jpg\n",
      "4\n",
      "processing image testing/walk/78.jpg\n",
      "4\n",
      "processing image testing/walk/2.jpg\n",
      "2\n",
      "processing image testing/walk/50.jpg\n",
      "4\n",
      "processing image testing/walk/44.jpg\n",
      "4\n",
      "processing image testing/walk/45.jpg\n",
      "4\n",
      "processing image testing/walk/3.jpg\n",
      "4\n",
      "processing image testing/walk/51.jpg\n",
      "4\n",
      "processing image testing/walk/79.jpg\n",
      "1\n",
      "processing image testing/walk/47.jpg\n",
      "4\n",
      "processing image testing/walk/53.jpg\n",
      "4\n",
      "processing image testing/walk/1.jpg\n",
      "3\n",
      "processing image testing/walk/52.jpg\n",
      "4\n",
      "processing image testing/walk/46.jpg\n",
      "4\n",
      "Time taken: 2.749321937561035\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "print(\"Time at start: \" + str((time.time() - t)))\n",
    "result = getAllImages('testing/walk')\n",
    "print(\"Time taken: \" + str((time.time() - t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fall: 11 14.102564102564102\n",
      "sit: 2 2.564102564102564\n",
      "stand: 9 11.538461538461538\n",
      "walk: 56 71.7948717948718\n"
     ]
    }
   ],
   "source": [
    "\n",
    "fall = 0\n",
    "walk = 0\n",
    "stand = 0\n",
    "sit = 0\n",
    "size = len(result)\n",
    "for i in result:\n",
    "    if i == 1:\n",
    "        fall += 1\n",
    "    elif i == 2:\n",
    "        sit += 1\n",
    "    elif i == 3:\n",
    "        stand += 1\n",
    "    elif i == 4:\n",
    "        walk += 1\n",
    "    else:\n",
    "        print(\"error\")\n",
    "        \n",
    "print('fall:', fall, (fall/size)*100)\n",
    "print('sit:', sit, (sit/size)*100)\n",
    "print('stand:', stand, (stand/size)*100)\n",
    "print('walk:', walk, (walk/size)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "#sample run\n",
    "print(predictPose('POSE22.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "def detectAllPlayers():\n",
    "    path = 'players/player'\n",
    "    detected_poses = []\n",
    "    player = 0\n",
    "    pose = 0\n",
    "    #to check if the player exists\n",
    "    playerExists = os.path.isfile(path+str(player)+'.jpg') \n",
    "\n",
    "    while playerExists:\n",
    "        pose = predictPose(path+str(player)+'.jpg')\n",
    "        detected_poses.append(pose)\n",
    "        player +=1\n",
    "        playerExists = os.path.isfile(path+str(player)+'.jpg')   \n",
    "\n",
    "    return detected_poses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player 0 is displaying pose 2\n",
      "Player 1 is displaying pose 6\n",
      "Player 2 is displaying pose 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for player in range(len(detectAllPlayers())):\n",
    "    print(\"Player\", player, \"is displaying pose\", detected_poses[player])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#game play\n",
    "def simonSays(correctPose):\n",
    "    playerPoses = detectAllPlayers()\n",
    "    for player in range(len(playerPoses)):\n",
    "        if playerPoses[player] == correctPose:\n",
    "            print(\"Player\", player, \"congrats! You've done the pose correctly\")\n",
    "        else:\n",
    "            print(\"Player\", player, \"uh-oh! You've done the pose incorrectly\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player 0 congrats! You've done the pose correctly\n",
      "Player 1 uh-oh! You've done the pose incorrectly\n",
      "Player 2 congrats! You've done the pose correctly\n"
     ]
    }
   ],
   "source": [
    "simonSays(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
