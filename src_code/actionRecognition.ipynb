{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data for the model\n",
    "\n",
    "There is too much data for us to read all at once in memory. We can use some built in functions in Keras to automatically process the data, generate a flow of batches from a directory, and also manipulate the images.\n",
    "\n",
    "### Image Manipulation\n",
    "\n",
    "Its usually a good idea to manipulate the images with rotation, resizing, and scaling so the model becomes more robust to different images that our data set doesn't have. We can use the **ImageDataGenerator** to do this automatically for us. Check out the documentation for a full list of all the parameters you can use here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_gen = ImageDataGenerator(#rotation_range=30, # rotate the image 30 degrees\n",
    "                               #width_shift_range=0.1, # Shift the pic width by a max of 10%\n",
    "                               #height_shift_range=0.1, # Shift the pic height by a max of 10%\n",
    "                               rescale=1/255, # Rescale the image by normalzing it.\n",
    "                               #shear_range=0.2, # Shear means cutting away part of the image (max 20%)\n",
    "                               #zoom_range=0.2, # Zoom in by 20% max\n",
    "                               horizontal_flip=True, # Allo horizontal flipping\n",
    "                               #fill_mode='nearest' # Fill in missing pixels with the nearest filled value\n",
    "                              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating many manipulated images from a directory\n",
    "\n",
    "\n",
    "In order to use .flow_from_directory, you must organize the images in sub-directories. This is an absolute requirement, otherwise the method won't work. The directories should only contain images of one class, so one folder per class of images.\n",
    "\n",
    "Structure Needed:\n",
    "\n",
    "* Image Data Folder\n",
    "    * Class 1\n",
    "        * 0.jpg\n",
    "        * 1.jpg\n",
    "        * ...\n",
    "    * Class 2\n",
    "        * 0.jpg\n",
    "        * 1.jpg\n",
    "        * ...\n",
    "    * ...\n",
    "    * Class n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 294 images belonging to 4 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras_preprocessing.image.DirectoryIterator at 0x7fb025397908>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#image_gen.flow_from_directory('positions/train')\n",
    "image_gen.flow_from_directory('actions/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 77 images belonging to 4 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras_preprocessing.image.DirectoryIterator at 0x7fb02950aa90>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_gen.flow_from_directory('actions/test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resizing Images\n",
    "\n",
    "Let's have Keras resize all the images to 150 pixels by 150 pixels once they've been manipulated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# width,height,channels\n",
    "image_shape = (150,150,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, Conv2D, MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=(3,3),input_shape=(150,150,3), activation='relu',))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=(3,3),input_shape=(150,150,3), activation='relu',))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=(3,3),input_shape=(150,150,3), activation='relu',))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# Dropouts help reduce overfitting by randomly turning neurons off during training.\n",
    "# Here we say randomly turn off 50% of neurons.\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Last layer, not binary!\n",
    "model.add(Dense(4))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', #not binary for the poses\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 148, 148, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 72, 72, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 34, 34, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 17, 17, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 18496)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               2367616   \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 516       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 2,424,452\n",
      "Trainable params: 2,424,452\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 294 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "\n",
    "train_image_gen = image_gen.flow_from_directory('actions/train',\n",
    "                                               target_size=image_shape[:2],\n",
    "                                               batch_size=batch_size,\n",
    "                                               class_mode='categorical') #catagoricalnfor the poses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DirectoryIterator' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-479723ebb1d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#print(next(train_image_gen))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_image_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'DirectoryIterator' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "#print(next(train_image_gen))\n",
    "#train_image_gen.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 77 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "test_image_gen = image_gen.flow_from_directory('actions/test',\n",
    "                                               target_size=image_shape[:2],\n",
    "                                               batch_size=batch_size,\n",
    "                                               class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fall': 0, 'sit': 1, 'stand': 2, 'walk': 3}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_image_gen.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "150/150 [==============================] - 97s 647ms/step - loss: 0.2215 - acc: 0.9299 - val_loss: 2.9458 - val_acc: 0.4286\n",
      "Epoch 2/10\n",
      "150/150 [==============================] - 96s 642ms/step - loss: 0.0769 - acc: 0.9772 - val_loss: 3.7612 - val_acc: 0.4545\n",
      "Epoch 3/10\n",
      "150/150 [==============================] - 93s 622ms/step - loss: 0.0353 - acc: 0.9892 - val_loss: 4.6552 - val_acc: 0.5325\n",
      "Epoch 4/10\n",
      "150/150 [==============================] - 104s 691ms/step - loss: 0.0233 - acc: 0.9925 - val_loss: 5.1413 - val_acc: 0.5325\n",
      "Epoch 5/10\n",
      "150/150 [==============================] - 99s 659ms/step - loss: 0.0272 - acc: 0.9937 - val_loss: 5.5583 - val_acc: 0.5195\n",
      "Epoch 6/10\n",
      "150/150 [==============================] - 96s 643ms/step - loss: 0.0099 - acc: 0.9979 - val_loss: 5.9112 - val_acc: 0.5195\n",
      "Epoch 7/10\n",
      "150/150 [==============================] - 97s 644ms/step - loss: 0.0167 - acc: 0.9971 - val_loss: 5.3903 - val_acc: 0.5584\n",
      "Epoch 8/10\n",
      "150/150 [==============================] - 97s 643ms/step - loss: 0.0059 - acc: 0.9983 - val_loss: 6.1311 - val_acc: 0.5325\n",
      "Epoch 9/10\n",
      "150/150 [==============================] - 94s 629ms/step - loss: 0.0172 - acc: 0.9958 - val_loss: 6.4711 - val_acc: 0.5455\n",
      "Epoch 10/10\n",
      "150/150 [==============================] - 94s 626ms/step - loss: 0.0163 - acc: 0.9968 - val_loss: 6.1810 - val_acc: 0.5455\n"
     ]
    }
   ],
   "source": [
    "results = model.fit_generator(train_image_gen,epochs=10,\n",
    "                              steps_per_epoch=150,\n",
    "                              validation_data=test_image_gen,\n",
    "                             validation_steps=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model0.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9288793102420609,\n",
       " 0.9771551723624098,\n",
       " 0.9888412017167382,\n",
       " 0.9922413793103448,\n",
       " 0.9935344827586207,\n",
       " 0.9978354978354979,\n",
       " 0.9969827586206896,\n",
       " 0.9982832618025751,\n",
       " 0.9956896551724138,\n",
       " 0.997413793052065]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.history['acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7faff14f0ac8>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(results.history['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = load_model('model0.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting on new images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fall': 0, 'sit': 1, 'stand': 2, 'walk': 3}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_image_gen.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(802, 350, 3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = cv2.imread('POSE3.jpg')\n",
    "v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "\n",
    "pose_file = 'testing/fall4.jpg'\n",
    "\n",
    "pose = image.load_img(pose_file, target_size=(150, 150))\n",
    "\n",
    "pose = image.img_to_array(pose)\n",
    "\n",
    "pose = np.expand_dims(pose, axis=0) #so the network can think its a batch of one image\n",
    "pose = pose/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_prob = new_model.predict(pose)\n",
    "new_model.predict_classes(pose) \n",
    "                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.3526377e-23 9.9999774e-01 4.9228033e-18 9.8221578e-12 1.7766470e-19\n",
      "  1.5273822e-19 3.9252950e-18 4.9790081e-18]]\n"
     ]
    }
   ],
   "source": [
    "# Output prediction\n",
    "print(prediction_prob) #how sure is it that the image belongs to the array it chose "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "#function to classify a pose\n",
    "def predictPose(imageName):\n",
    "    #resize the image since the model is trained with 150 by 150 images\n",
    "    pose = image.load_img(imageName, target_size=(150,150))\n",
    "    pose = image.img_to_array(pose)\n",
    "    pose = np.expand_dims(pose, axis=0)\n",
    "    pose = pose/255\n",
    "    \n",
    "    #actual classification\n",
    "    prediction_prob = new_model.predict(pose)\n",
    "    #returns which pose\n",
    "    poseNumber = new_model.predict_classes(pose)\n",
    "    #print(prediction_prob)\n",
    "    return poseNumber[0]+1\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAllImages(rootdir):\n",
    "    \"\"\"Gets all of the images in a folder and runs them through OpenPose.\n",
    "    You just need to run this function with the folder of pose images\"\"\"\n",
    "    path, dirs, files = next(os.walk(rootdir))\n",
    "    file_count = len(files) - 1\n",
    "    poseList = []\n",
    "    action = 0\n",
    "    actionList = []\n",
    "    i = 0\n",
    "    for subdir, dirs, files in os.walk(rootdir):\n",
    "        for file in files:\n",
    "            # print os.path.join(subdir, file)\n",
    "            filepath = subdir + os.sep + file\n",
    "\n",
    "            if filepath.endswith(\".jpg\"):\n",
    "                i += 1\n",
    "                #printProgressBar(i + 1, file_count, prefix='Progress:', suffix='Complete', length=50)\n",
    "                print(\"processing image \"+ str(i))\n",
    "                action = predictPose(filepath)\n",
    "                print(action)\n",
    "                actionList.append(action)\n",
    "    return actionList\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time at start: 4.887580871582031e-05\n",
      "processing image 1\n",
      "2\n",
      "processing image 2\n",
      "3\n",
      "processing image 3\n",
      "4\n",
      "processing image 4\n",
      "3\n",
      "processing image 5\n",
      "3\n",
      "processing image 6\n",
      "4\n",
      "processing image 7\n",
      "1\n",
      "processing image 8\n",
      "1\n",
      "processing image 9\n",
      "1\n",
      "processing image 10\n",
      "2\n",
      "processing image 11\n",
      "1\n",
      "processing image 12\n",
      "4\n",
      "processing image 13\n",
      "1\n",
      "processing image 14\n",
      "2\n",
      "processing image 15\n",
      "4\n",
      "processing image 16\n",
      "3\n",
      "processing image 17\n",
      "1\n",
      "processing image 18\n",
      "2\n",
      "processing image 19\n",
      "1\n",
      "processing image 20\n",
      "4\n",
      "processing image 21\n",
      "2\n",
      "processing image 22\n",
      "2\n",
      "processing image 23\n",
      "1\n",
      "processing image 24\n",
      "1\n",
      "processing image 25\n",
      "1\n",
      "processing image 26\n",
      "1\n",
      "processing image 27\n",
      "2\n",
      "processing image 28\n",
      "2\n",
      "processing image 29\n",
      "4\n",
      "processing image 30\n",
      "4\n",
      "processing image 31\n",
      "1\n",
      "processing image 32\n",
      "1\n",
      "processing image 33\n",
      "4\n",
      "processing image 34\n",
      "2\n",
      "processing image 35\n",
      "1\n",
      "processing image 36\n",
      "1\n",
      "processing image 37\n",
      "3\n",
      "processing image 38\n",
      "3\n",
      "processing image 39\n",
      "1\n",
      "processing image 40\n",
      "2\n",
      "processing image 41\n",
      "2\n",
      "processing image 42\n",
      "4\n",
      "processing image 43\n",
      "2\n",
      "processing image 44\n",
      "2\n",
      "processing image 45\n",
      "2\n",
      "processing image 46\n",
      "2\n",
      "processing image 47\n",
      "2\n",
      "processing image 48\n",
      "2\n",
      "processing image 49\n",
      "1\n",
      "processing image 50\n",
      "4\n",
      "processing image 51\n",
      "3\n",
      "processing image 52\n",
      "4\n",
      "processing image 53\n",
      "4\n",
      "processing image 54\n",
      "1\n",
      "processing image 55\n",
      "1\n",
      "processing image 56\n",
      "3\n",
      "processing image 57\n",
      "1\n",
      "processing image 58\n",
      "1\n",
      "processing image 59\n",
      "4\n",
      "processing image 60\n",
      "4\n",
      "processing image 61\n",
      "1\n",
      "processing image 62\n",
      "4\n",
      "processing image 63\n",
      "2\n",
      "processing image 64\n",
      "1\n",
      "processing image 65\n",
      "4\n",
      "processing image 66\n",
      "2\n",
      "processing image 67\n",
      "1\n",
      "processing image 68\n",
      "1\n",
      "processing image 69\n",
      "1\n",
      "processing image 70\n",
      "1\n",
      "processing image 71\n",
      "1\n",
      "processing image 72\n",
      "3\n",
      "processing image 73\n",
      "2\n",
      "processing image 74\n",
      "1\n",
      "processing image 75\n",
      "2\n",
      "processing image 76\n",
      "2\n",
      "processing image 77\n",
      "1\n",
      "processing image 78\n",
      "1\n",
      "processing image 79\n",
      "2\n",
      "processing image 80\n",
      "1\n",
      "processing image 81\n",
      "3\n",
      "processing image 82\n",
      "1\n",
      "processing image 83\n",
      "1\n",
      "processing image 84\n",
      "1\n",
      "processing image 85\n",
      "1\n",
      "processing image 86\n",
      "3\n",
      "processing image 87\n",
      "1\n",
      "processing image 88\n",
      "2\n",
      "processing image 89\n",
      "2\n",
      "processing image 90\n",
      "4\n",
      "processing image 91\n",
      "2\n",
      "processing image 92\n",
      "1\n",
      "processing image 93\n",
      "1\n",
      "processing image 94\n",
      "1\n",
      "processing image 95\n",
      "1\n",
      "processing image 96\n",
      "1\n",
      "processing image 97\n",
      "1\n",
      "processing image 98\n",
      "4\n",
      "processing image 99\n",
      "2\n",
      "processing image 100\n",
      "3\n",
      "processing image 101\n",
      "1\n",
      "processing image 102\n",
      "1\n",
      "processing image 103\n",
      "1\n",
      "processing image 104\n",
      "4\n",
      "processing image 105\n",
      "2\n",
      "processing image 106\n",
      "1\n",
      "processing image 107\n",
      "1\n",
      "processing image 108\n",
      "1\n",
      "processing image 109\n",
      "1\n",
      "processing image 110\n",
      "1\n",
      "processing image 111\n",
      "2\n",
      "processing image 112\n",
      "2\n",
      "processing image 113\n",
      "1\n",
      "processing image 114\n",
      "1\n",
      "processing image 115\n",
      "2\n",
      "processing image 116\n",
      "1\n",
      "processing image 117\n",
      "1\n",
      "processing image 118\n",
      "1\n",
      "processing image 119\n",
      "4\n",
      "processing image 120\n",
      "2\n",
      "processing image 121\n",
      "1\n",
      "processing image 122\n",
      "2\n",
      "processing image 123\n",
      "2\n",
      "processing image 124\n",
      "1\n",
      "processing image 125\n",
      "1\n",
      "processing image 126\n",
      "2\n",
      "processing image 127\n",
      "4\n",
      "processing image 128\n",
      "2\n",
      "processing image 129\n",
      "2\n",
      "processing image 130\n",
      "1\n",
      "processing image 131\n",
      "1\n",
      "processing image 132\n",
      "1\n",
      "processing image 133\n",
      "1\n",
      "processing image 134\n",
      "1\n",
      "processing image 135\n",
      "1\n",
      "processing image 136\n",
      "4\n",
      "processing image 137\n",
      "1\n",
      "processing image 138\n",
      "1\n",
      "processing image 139\n",
      "2\n",
      "processing image 140\n",
      "1\n",
      "processing image 141\n",
      "1\n",
      "processing image 142\n",
      "1\n",
      "processing image 143\n",
      "1\n",
      "processing image 144\n",
      "2\n",
      "processing image 145\n",
      "1\n",
      "processing image 146\n",
      "4\n",
      "processing image 147\n",
      "3\n",
      "processing image 148\n",
      "1\n",
      "processing image 149\n",
      "1\n",
      "processing image 150\n",
      "1\n",
      "processing image 151\n",
      "3\n",
      "processing image 152\n",
      "1\n",
      "Time taken: 5.554337024688721\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "print(\"Time at start: \" + str((time.time() - t)))\n",
    "result = getAllImages('testing/fall')\n",
    "print(\"Time taken: \" + str((time.time() - t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fall: 76 50.0\n",
      "sit: 39 25.657894736842106\n",
      "stand: 14 9.210526315789473\n",
      "walk: 23 15.131578947368421\n"
     ]
    }
   ],
   "source": [
    "\n",
    "fall = 0\n",
    "walk = 0\n",
    "stand = 0\n",
    "sit = 0\n",
    "size = len(result)\n",
    "for i in result:\n",
    "    if i == 1:\n",
    "        fall += 1\n",
    "    elif i == 2:\n",
    "        sit += 1\n",
    "    elif i == 3:\n",
    "        stand += 1\n",
    "    elif i == 4:\n",
    "        walk += 1\n",
    "    else:\n",
    "        print(\"error\")\n",
    "        \n",
    "print('fall:', fall, (fall/size)*100)\n",
    "print('sit:', sit, (sit/size)*100)\n",
    "print('stand:', stand, (stand/size)*100)\n",
    "print('walk:', walk, (walk/size)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "#sample run\n",
    "print(predictPose('POSE22.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "def detectAllPlayers():\n",
    "    path = 'players/player'\n",
    "    detected_poses = []\n",
    "    player = 0\n",
    "    pose = 0\n",
    "    #to check if the player exists\n",
    "    playerExists = os.path.isfile(path+str(player)+'.jpg') \n",
    "\n",
    "    while playerExists:\n",
    "        pose = predictPose(path+str(player)+'.jpg')\n",
    "        detected_poses.append(pose)\n",
    "        player +=1\n",
    "        playerExists = os.path.isfile(path+str(player)+'.jpg')   \n",
    "\n",
    "    return detected_poses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player 0 is displaying pose 2\n",
      "Player 1 is displaying pose 6\n",
      "Player 2 is displaying pose 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for player in range(len(detectAllPlayers())):\n",
    "    print(\"Player\", player, \"is displaying pose\", detected_poses[player])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#game play\n",
    "def simonSays(correctPose):\n",
    "    playerPoses = detectAllPlayers()\n",
    "    for player in range(len(playerPoses)):\n",
    "        if playerPoses[player] == correctPose:\n",
    "            print(\"Player\", player, \"congrats! You've done the pose correctly\")\n",
    "        else:\n",
    "            print(\"Player\", player, \"uh-oh! You've done the pose incorrectly\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player 0 congrats! You've done the pose correctly\n",
      "Player 1 uh-oh! You've done the pose incorrectly\n",
      "Player 2 congrats! You've done the pose correctly\n"
     ]
    }
   ],
   "source": [
    "simonSays(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
